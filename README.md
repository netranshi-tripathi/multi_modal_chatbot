# ğŸ§  Multi-Model LLM Chatbot using Ollama

A dynamic, modular chatbot framework powered by multiple lightweight and efficient LLMs (such as **Gemma** and **TinyLlama**) using the **Ollama** ecosystem. The system intelligently routes user queries to the most suitable model based on intent, ensuring accurate, low-latency, and context-aware conversations.

---

## ğŸš€ Features

- ğŸ” **Multi-Model Inference**: Seamlessly integrates multiple local models (Gemma, TinyLlama, etc.) with intelligent routing based on query type.
- âš™ï¸ **Dynamic Model Invocation**: Custom API-based system dynamically selects the appropriate LLM at runtime.
- ğŸ§© **Modular & Extensible**: Designed for scalability and rapid addition of new models or functionalities without architectural overhaul.
- âš¡ **Low-Latency**: Local model execution with minimal response lag using Ollama backend.
- âŒ **No RAG Dependency**: Built purely on LLM generation capabilities, without requiring Retrieval-Augmented Generation pipelines.

---

## ğŸ§° Tech Stack

- **Backend**: Python, FastAPI
- **Model Runtime**: [Ollama](https://ollama.com/)
- **Models Used**: Gemma, TinyLlama (additional models pluggable)
- **API Handling**: Custom dynamic routing
- **Local Execution**: CPU/GPU via Ollama environment

---

## ğŸ“ Project Structure

```bash
â”œâ”€â”€ app.py                # FastAPI app to handle requests
â”œâ”€â”€ model_router.py       # Logic to dynamically invoke models
â”œâ”€â”€ prompts/              # Prompt templates for different intents
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md             # Project documentation
